{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "softlab-full.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_u_Rz2Cdbv2X"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jay22519/Automatic-Answer-checker-/blob/main/softlab_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u_Rz2Cdbv2X"
      },
      "source": [
        "# Necessary installs and imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4hbFozwb2M5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420721c7-1442-483e-d2ab-05b412cb7bcc"
      },
      "source": [
        "%%time\n",
        "\n",
        "# For OCR\n",
        "%pip install -q autocorrect\n",
        "%pip install -q azure-cognitiveservices-vision-computervision\n",
        "%pip install -q pillow\n",
        "\n",
        "from autocorrect import Speller\n",
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes, VisualFeatureTypes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "from getpass import getpass\n",
        "\n",
        "subscription_key = getpass(\"Enter the Azure API key: \") #\"5d1a1d55d59b43369286d49fe032e71c\" # Azure API key\n",
        "endpoint = \"https://autoanschecker.cognitiveservices.azure.com/\"\n",
        "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\n",
        "\n",
        "# For NLP\n",
        "%pip install -q gensim\n",
        "%pip install -q nltk\n",
        "%pip install -q sklearn\n",
        "%pip install -q tensorflow_hub\n",
        "%pip install -q pyemd\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "from pyemd import emd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "from array import array\n",
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "import shutil\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▌                               | 10kB 17.7MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 23.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30kB 12.9MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 9.2MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 5.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61kB 6.0MB/s eta 0:00:01\r\u001b[K     |███▊                            | 71kB 6.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 6.9MB/s eta 0:00:01\r\u001b[K     |████▊                           | 92kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 112kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 122kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 133kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 143kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 153kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 163kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 184kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 194kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 204kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 215kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 225kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 235kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 245kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 256kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 266kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 276kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 286kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 296kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 307kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 317kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 327kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 337kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 348kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 358kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 368kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 378kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 389kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 399kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 409kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 419kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 430kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 440kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 450kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 460kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 471kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 481kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 491kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 501kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 512kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 522kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 532kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 542kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 552kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 563kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 573kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 583kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 593kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 604kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 614kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 624kB 6.4MB/s \n",
            "\u001b[?25h  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 92kB 4.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.0MB/s \n",
            "\u001b[?25hEnter the Azure API key: ··········\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "CPU times: user 2.79 s, sys: 553 ms, total: 3.34 s\n",
            "Wall time: 42.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97QBu3vV8fhQ"
      },
      "source": [
        "# OCR code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK6zDLbmK95t"
      },
      "source": [
        "\"\"\"\n",
        "Takes a list of uploaded filenames and returns list of file_text\n",
        "\"\"\"\n",
        "def get_ocr_text(uploaded_files):\n",
        "    to_return = []\n",
        "\n",
        "    if not uploaded_files:\n",
        "        print(\"No files uploaded!\")\n",
        "    \n",
        "    check = Speller(lang='en')\n",
        "    for i, image in enumerate(uploaded_files, 1):\n",
        "        # Open the image\n",
        "        local_image_handwritten = open(image, \"rb\")\n",
        "\n",
        "        # Call API with image and raw response (allows you to get the operation location)\n",
        "        recognize_handwriting_results = computervision_client.read_in_stream(local_image_handwritten, raw=True)\n",
        "\n",
        "        # Get the operation location (URL with ID as last appendage)\n",
        "        operation_location_local = recognize_handwriting_results.headers[\"Operation-Location\"]\n",
        "        # Take the ID off and use to get results\n",
        "        operation_id_local = operation_location_local.split(\"/\")[-1]\n",
        "\n",
        "        # Call the \"GET\" API and wait for the retrieval of the results\n",
        "        while True:\n",
        "            recognize_handwriting_result = computervision_client.get_read_result(operation_id_local)\n",
        "            if recognize_handwriting_result.status not in ['notStarted', 'running']:\n",
        "                break\n",
        "            time.sleep(1)\n",
        "\n",
        "        # print(f\"===== Extracted text from image #{i}: =====\")\n",
        "        lines = []\n",
        "        # Print results, line by line\n",
        "        if recognize_handwriting_result.status == OperationStatusCodes.succeeded:\n",
        "            for text_result in recognize_handwriting_result.analyze_result.read_results:\n",
        "                for line in text_result.lines:\n",
        "                    # print(line.text, sep=' ')        # original OCR'ed line\n",
        "                    # only autocorrect words which aren't abbreviations.\n",
        "                    corrected = [word if bool(re.search(\"[A-Z]+\", word)) else check(word) for word in line.text.split() ]\n",
        "                    # print(\" \".join(corrected))\n",
        "                    lines.append(\" \".join(corrected))\n",
        "\n",
        "        to_return.append(\" \".join(lines))\n",
        "        # print()\n",
        "    return to_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpcNsdzSb4Hm"
      },
      "source": [
        "# NLP Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2BoN8HsegiN",
        "outputId": "5544d5ef-4887-49f1-8bc8-0c50ea88918b"
      },
      "source": [
        "%%time\n",
        "path_to_saved_model = gensim.downloader.load('word2vec-google-news-300', return_path=True)\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(path_to_saved_model, binary=True)  \n",
        "model.init_sims(replace=True)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7ff04937e8d0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUv8i9ONru5x",
        "outputId": "a2474cbe-ab71-4ce9-8004-286b11e3f322"
      },
      "source": [
        "%%time\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 14.3 s, sys: 7.01 s, total: 21.4 s\n",
            "Wall time: 25.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH5Ee_FmsAhp"
      },
      "source": [
        "def cos_sim(input_vectors):\n",
        "    similarity = cosine_similarity(input_vectors)\n",
        "    return similarity\n",
        "\n",
        "negative = [\"not\" , \"without\", \"against\", \"bad\", \"useless\", \"no\", \"dislike\", \"hate\"]\n",
        "\n",
        "def semantic_similarity(actual_answer , given_answer) :\n",
        "    actual = actual_answer.lower().split(\".\")\n",
        "    given = given_answer.lower().split(\".\")\n",
        "    \n",
        "    sim_checker = actual \n",
        "    \n",
        "    not_matching_semantics = list()\n",
        "    \n",
        "    semantic_1 = 0   # Actual_answer\n",
        "    semantic_2 = 0   # Given_answer\n",
        "    \n",
        "    actual_embed_list = list()\n",
        "    given_embed_list = list()\n",
        "    \n",
        "    for z in range(len(actual)) :\n",
        "        list_actual = list()  \n",
        "        list_actual.append(actual[z])\n",
        "        actual_embed_list.append(embed(list_actual))\n",
        "        #print(actual_embed_list[z].shape)\n",
        "    \n",
        "    for z in range(len(given)) :    \n",
        "        semantic_1 = 0\n",
        "        semantic_2 = 0 \n",
        "        list_given = list()\n",
        "        list_given.append(given[z])\n",
        "        embed_z = embed(list_given)\n",
        "        \n",
        "        sim_check = sim_checker.copy() \n",
        "        sim_check.append(given[z]) \n",
        "        \n",
        "        sen_em = embed(sim_check)\n",
        "        \n",
        "        similarity_matrix = cos_sim(np.array(sen_em))\n",
        "        \n",
        "        similarity_matrix_df = pd.DataFrame(similarity_matrix) \n",
        "        \n",
        "        cos_list = list(similarity_matrix_df[len(similarity_matrix_df) - 1]) \n",
        "        cos_list = cos_list[:len(cos_list)-1]\n",
        "        #print(cos_list)\n",
        "        \n",
        "        index = cos_list.index(max(cos_list))\n",
        "        \n",
        "        actual_check = actual[index]\n",
        "        actual_check = actual_check.split()\n",
        "        for i in range(len(actual_check) - 1) :\n",
        "            if(actual_check[i] in negative and actual_check[i+1] in negative) :\n",
        "                semantic_1 += 1 \n",
        "            elif(actual_check[i] in negative and actual_check[i+1] not in negative) :\n",
        "                semantic_1 -= 1\n",
        "\n",
        "        answer_given = given[z].split()\n",
        "        for i in range(len(answer_given) - 1) :\n",
        "            if(answer_given[i] in negative and answer_given[i+1] in negative) :\n",
        "                semantic_2 += 1 \n",
        "            elif(answer_given[i] in negative and answer_given[i+1] not in negative) :\n",
        "                semantic_2 -= 1 \n",
        "        \n",
        "        if(semantic_1 == 0 and semantic_2 == 0) :\n",
        "            \n",
        "            \"\"\"\n",
        "            Well and good\n",
        "            \"\"\"\n",
        "        elif(semantic_1 < 0  and semantic_2 >= 0) :\n",
        "            not_matching_semantics.append(list([actual[index],given[z]]))\n",
        "            embed_z*=(-1)\n",
        "\n",
        "        elif(semantic_1 >= 0 and semantic_2 < 0 ) :\n",
        "            not_matching_semantics.append(list([actual[index],given[z]]))\n",
        "            embed_z*=(-1)\n",
        "        \n",
        "        #print(semantic_1,semantic_2,actual[index],given[z])\n",
        "        given_embed_list.append(embed_z)\n",
        "    \n",
        "    #print(np.array(actual_embed_list).shape)\n",
        "    actual_embed = actual_embed_list[0] \n",
        "    #print(actual_embed.shape) \n",
        "    \n",
        "    for i in range(len(actual_embed_list)-1) :\n",
        "        #print(actual_embed_list[i+1].shape)\n",
        "        actual_embed += actual_embed_list[i+1]\n",
        "        \n",
        "    given_embed = given_embed_list[0] \n",
        "    for i in range(len(given_embed_list) - 1) :\n",
        "        given_embed += given_embed_list[i+1] \n",
        "            \n",
        "    actual_embed = np.array(actual_embed).reshape(512)\n",
        "    given_embed = np.array(given_embed).reshape(512) \n",
        "    sem_checker = list([actual_embed,given_embed]) \n",
        "    answer = pd.DataFrame(cos_sim(sem_checker))\n",
        "        \n",
        "    return not_matching_semantics , answer[0][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_oGCLP-tGGd"
      },
      "source": [
        "def WMD(actual_answer, given_answer, model) :\n",
        "    actual_answer = actual_answer.lower().split()\n",
        "    actual_answer = [w for w in actual_answer if w not in stop_words]\n",
        "    \n",
        "    given_answer = given_answer.lower().split()\n",
        "    given_answer = [w for w in given_answer if w not in stop_words]\n",
        "    \n",
        "    return model.wmdistance(given_answer,actual_answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrGpKXAftR0O"
      },
      "source": [
        "def score(given_answer, actual_answer, model) :\n",
        "    given_answer1 = given_answer[:]\n",
        "    actual_answer1 = actual_answer[:]\n",
        "    \n",
        "    given_answer2 = given_answer[:]\n",
        "    actual_answer2 = actual_answer[:]\n",
        "\n",
        "    not_matching , similarity = semantic_similarity(actual_answer1, given_answer1)\n",
        "    distance = WMD(actual_answer2, given_answer2, model)\n",
        "    \n",
        "    if(similarity > 0) :\n",
        "        if(distance == 0) :\n",
        "            return 1 \n",
        "        return similarity/distance\n",
        "    else :\n",
        "        return -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnx_1xNeSS2U"
      },
      "source": [
        "# Run with Flask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rYFnOGNS6nv"
      },
      "source": [
        "%pip install -q flask \n",
        "%pip install -q flask_ngrok \n",
        "%pip install -q werkzeug \n",
        "%pip install -q flask_wtf\n",
        "\n",
        "%cd /content/\n",
        "!git clone https://github.com/Jay22519/Automatic-Answer-checker-\n",
        "%cd Automatic-Answer-checker-\n",
        "!printf \"Currently in directory: \"; pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAcw6hNJSSL-"
      },
      "source": [
        "from flask import Flask, redirect, render_template, request, make_response, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from werkzeug.utils import secure_filename\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "UPLOAD_FOLDER = '/content/upload'\n",
        "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
        "\n",
        "@app.route(\"/\")\n",
        "@app.route(\"/index\")\n",
        "def home():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route(\"/live-demo\")\n",
        "def live_demo():\n",
        "    return render_template(\"live-demo.html\")\n",
        "\n",
        "@app.route(\"/result\", methods = ['GET', 'POST'])\n",
        "def result():\n",
        "    print(\"******* Inside RESULT **********\")\n",
        "    if request.method == 'POST':\n",
        "        print(\"IT'S POST MALONE: \", request.files)\n",
        "\n",
        "        teacher_file = request.files.get('file1', None)\n",
        "        student_files = request.files.getlist('file2', None)\n",
        "\n",
        "        # print(\"Got files: \", file1, file2)\n",
        "\n",
        "        if teacher_file and student_files:\n",
        "            teacher_file.save(teacher_file.filename)\n",
        "\n",
        "            for file_ in student_files:\n",
        "                file_.save(file_.filename)\n",
        "\n",
        "            teacher_text = get_ocr_text([teacher_file.filename])[0]\n",
        "            students_texts = get_ocr_text([f.filename for f in student_files])\n",
        "            \n",
        "            resulting_scores = [score(teacher_text, stud, model) for stud in students_texts]            \n",
        "            print(\"res is of length\", len(resulting_scores))\n",
        "\n",
        "            # return redirect(url_for(\"\"))\n",
        "            return {\n",
        "                \"Student Grades\": [{\n",
        "                    \"Filename\": student_files[i].filename,\n",
        "                    \"Score\": r,\n",
        "                    \"Percentage\": (r+1)*50\n",
        "                } for i, r in enumerate(resulting_scores)],\n",
        "            }\n",
        "    return make_response(\"Invalid somehow!\")#redirect(url_for('home'))\n",
        "\n",
        "# @app.route(\"/results\")\n",
        "\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJgZq1ByO5c3",
        "outputId": "c7c0da3d-4f58-46b9-e494-d3ba01829faf"
      },
      "source": [
        "%cd /content\n",
        "%rm -r Automatic-Answer-checker-\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "rm: cannot remove 'Automatic-Answer-checker-': No such file or directory\n",
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}